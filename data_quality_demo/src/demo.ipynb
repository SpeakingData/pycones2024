{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Process Bronze-to-Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5349b05a-3724-437d-bd18-4f27978932b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window, Column, DataFrame, SparkSession\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Add console handler for Databricks output\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Ensure no duplicate handlers\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_table(schema: str, table_name: str):\n",
    "    salesConstraints = f\"\"\"{{\n",
    "        \"tableName\": \"silver_sales\",\n",
    "        \"constraints\": [\n",
    "            \"ALTER TABLE {schema}.silver_sales ALTER COLUMN sales_id SET NOT NULL;\",\n",
    "            \"ALTER TABLE {schema}.silver_sales ALTER COLUMN client_id SET NOT NULL;\",\n",
    "            \"ALTER TABLE {schema}.silver_sales ALTER COLUMN product_id SET NOT NULL;\",\n",
    "            \"ALTER TABLE {schema}.silver_sales ADD CONSTRAINT dateWithinRange  CHECK (date between '2020-01-01' and '2025-01-01');\",\n",
    "            \"ALTER TABLE {schema}.silver_sales ADD CONSTRAINT validQuantity  CHECK (quantity > 0);\"\n",
    "        ]\n",
    "    }}\"\"\"\n",
    "\n",
    "    df = spark.read.json(spark.sparkContext.parallelize([salesConstraints])).select(\"tableName\", explode(\"constraints\"))\n",
    "    df.write.mode(\"overwrite\").saveAsTable(f\"{schema}.{table_name}\")\n",
    "\n",
    "    return salesConstraints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_table_constaints(source_constraints_tablename: str, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Apply table-specific SQL constraints to the given Delta table.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        source_constraints_tablename (str): The table containing the constraints definitions.\n",
    "        table_name (str): The table to which constraints are being applied.\n",
    "\n",
    "    Returns:\n",
    "        None: This function applies constraints directly to the table using SQL statements.\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying table constraints to table: %s\", table_name)\n",
    "\n",
    "    # Read the constraints from the source constraints table\n",
    "    constraints_df = spark.read.table(source_constraints_tablename)\n",
    "    \n",
    "    # Filter constraints relevant to the target table\n",
    "    table_constraints = constraints_df.filter(f\"tableName == '{table_name}'\")\n",
    "\n",
    "    # Apply each constraint via SQL execution\n",
    "    for row in table_constraints.collect():\n",
    "        try:\n",
    "            logger.info(\"Applying constraint: %s\", row[1])\n",
    "            spark.sql(row[1])\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error applying constraint %s to table %s: %s\", row[1], table_name, str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_constraints_conditions(schema: str, table_name: str) -> Column:\n",
    "    \"\"\"\n",
    "    Generate a composite filter condition for validating table constraints, including non-nullable columns and custom constraints.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        schema (str): The schema of the table.\n",
    "        table_name (str): The name of the table for which constraints are generated.\n",
    "\n",
    "    Returns:\n",
    "        Column: A PySpark Column object representing the composite filter condition for the constraints.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating table constraint conditions for table: %s.%s\", schema, table_name)\n",
    "\n",
    "    # Identify non-nullable fields and generate a filter for them\n",
    "    nullable_filters = [\n",
    "        f\"{x.name} IS NOT NULL\" \n",
    "        for x in spark.table(f\"{schema}.{table_name}\").schema.fields if not x.nullable\n",
    "    ]\n",
    "    logger.info(\"Non-nullable field filters: %s\", nullable_filters)\n",
    "\n",
    "    # Retrieve custom table constraints (from Delta properties)\n",
    "    constraints_df = (\n",
    "        spark.sql(f\"SHOW TBLPROPERTIES {schema}.{table_name}\")\n",
    "        .filter(F.col(\"key\").startswith(\"delta.cons\"))\n",
    "        .select(\"value\")\n",
    "    )\n",
    "\n",
    "    # Collect the constraints from the table properties\n",
    "    constraints_filter = [c[0] for c in constraints_df.collect()]\n",
    "    logger.info(\"Custom constraints from TBLPROPERTIES: %s\", constraints_filter)\n",
    "\n",
    "    # Combine nullable filters and custom constraints into a single condition\n",
    "    constraints = nullable_filters + constraints_filter\n",
    "\n",
    "    # Reduce the list of constraints to a composite filter condition using logical OR\n",
    "    # If any constraint fails (is false), we want to quarantine the record\n",
    "    combined_condition = reduce(lambda x, y: x | ~F.expr(y), constraints, F.lit(False))\n",
    "    logger.info(\"Combined constraint condition generated.\")\n",
    "\n",
    "    return combined_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table(\"data_quality_demo.tableconstraints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_merge_into_delta(\n",
    "    batch_df: DataFrame, \n",
    "    batch_id: int, \n",
    "    table_schema: str, \n",
    "    table_name: str, \n",
    "    ids: list, \n",
    "    max_col: str = None, \n",
    "    quarantine_schema: str = \"quarantine\", \n",
    "    table_constraints_name: str = \"tableConstraints\"\n",
    ") -> None:\n",
    "    logger.info(\"Starting stream_merge_into_delta for table: %s.%s\", table_schema, table_name)\n",
    "    \n",
    "    # Step 1: Dedupe input batch, use max_col if provided\n",
    "    if max_col:\n",
    "        logger.info(\"Deduplication using max column: %s\", max_col)\n",
    "        w = Window.partitionBy(ids).orderBy(col(max_col).desc())\n",
    "        df_dedupe = (\n",
    "            batch_df\n",
    "            .withColumn(\"rank\", F.rank().over(w))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Deduplication using primary keys: %s\", ids)\n",
    "        df_dedupe = batch_df.dropDuplicates(ids)\n",
    "\n",
    "    # Step 2: Check if Delta table exists\n",
    "    if spark.catalog.tableExists(f\"{table_schema}.{table_name}\"):\n",
    "        logger.info(\"Delta table exists: %s.%s\", table_schema, table_name)\n",
    "        delta = DeltaTable.forName(spark, f\"{table_schema}.{table_name}\")\n",
    "    else:\n",
    "        # Step 3: Create Delta table if it doesn't exist\n",
    "        logger.info(\"Creating Delta table: %s.%s\", table_schema, table_name)\n",
    "        delta = (DeltaTable.create(spark)\n",
    "                 .tableName(f\"{table_schema}.{table_name}\")\n",
    "                 .addColumns(df_dedupe.schema)\n",
    "                 .execute())\n",
    "        \n",
    "        # Apply table constraints after table creation\n",
    "        apply_table_constaints(f\"{table_schema}.{table_constraints_name}\", table_name)\n",
    "\n",
    "    # Step 5: Quarantine invalid records based on table constraints\n",
    "    constraints_conditions = get_table_constraints_conditions(table_schema, table_name)\n",
    "    \n",
    "    quarantine_records = df_dedupe.filter(constraints_conditions)\n",
    "    valid_records = df_dedupe.filter(~constraints_conditions)\n",
    "\n",
    "    # Write quarantine records to the specified quarantine schema\n",
    "    quarantine_records.write.mode(\"append\").saveAsTable(f\"{table_schema}_{quarantine_schema}.{table_name}\")\n",
    "    \n",
    "    logger.info(\"Total Records: %d\", df_dedupe.count())\n",
    "    logger.info(\"Records moved to quarantine: %d\", quarantine_records.count())\n",
    "    logger.info(\"Valid Records: %d\", valid_records.count())\n",
    "\n",
    "    # Step 6: Create merge condition based on primary keys (ids)\n",
    "    condition = \" AND \".join(f\"l.{c} = r.{c}\" for c in ids)\n",
    "    logger.info(\"Merge condition: %s\", condition)\n",
    "\n",
    "    # Step 7: Perform the merge operation into Delta table\n",
    "    merge = (\n",
    "        delta.alias(\"l\")\n",
    "        .merge(valid_records.alias(\"r\"), condition)\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "    )\n",
    "    \n",
    "    merge.execute()\n",
    "    logger.info(\"Merge operation completed for table: %s.%s\", table_schema, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_table ='bronze_data_quality'\n",
    "schema = 'data_quality_demo'\n",
    "\n",
    "silver_schema = 'demo'\n",
    "silver_table ='silver_sales'\n",
    "checkpoint_path = 'tmp/testa/_checkpoints'\n",
    "cols = ['date','sales_id', 'client_id', 'product_id', 'quantity','sale_amount']\n",
    "ids = ['sales_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schemas & Constraint table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337ad734-60af-40f9-b079-b4f7b9611829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema}_quarantine\")\n",
    "\n",
    "table_constraints_name = 'tableConstraints'\n",
    "create_constraints_table(silver_schema, f\"{table_constraints_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Bronze table & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.table(f\"{schema}.{bronze_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "            df\n",
    "            .select(cols)\n",
    "            .writeStream\n",
    "            .format('delta')\n",
    "            .option(\"checkpointLocation\", f\"{checkpoint_path}/{silver_table}/\")\n",
    "            .foreachBatch(lambda batch_df, batch_id: stream_merge_into_delta(batch_df=batch_df, batch_id=batch_id, table_schema=silver_schema, table_name=silver_table, ids=ids, max_col=None))\n",
    "            .trigger(availableNow=True)\n",
    "            .start()\n",
    "        )\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table(f\"{silver_schema}.{silver_table}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table(f\"{silver_schema}_quarantine.{silver_table}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "spark.sql(f\"DROP SCHEMA  IF EXISTS {silver_schema} CASCADE\")\n",
    "spark.sql(f\"DROP SCHEMA  IF EXISTS {silver_schema}_quarantine CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "0_initialize_env",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
